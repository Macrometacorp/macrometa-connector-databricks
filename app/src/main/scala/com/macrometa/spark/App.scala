/*
 * This Scala source file was generated by the Gradle 'init' task.
 */
package com.macrometa.spark

import org.apache.spark.sql.functions.rand
import org.apache.spark.sql.{SaveMode, SparkSession}

object App {
  def main(args: Array[String]): Unit = {
    val apikey = "apikey TBHFGTw30RNa6dbixEn9Hcw.data.V6tTAkJgLbHS4nMFPJOr4qExXo22b8UhS9KhpGQdhH4k5F14l1BOVqsQuWkCLWbL784614"
    val federation = "devsuccess.eng.macrometa.io"
    val fabric = "_system"
    val collection = "numbers2"
    val batchSize = 10
    val query = s"FOR doc IN $collection RETURN doc"

    val spark = SparkSession.builder().master("local[*]").getOrCreate()

    //This part is to read from the collection named numbers
    val inputDF = spark.read
      .format("macrometa")
      .option("federation", federation)
      .option("apiKey", apikey)
      .option("fabric", fabric)
      .option("collection", collection)
      .option("batchSize", batchSize)
      .option("query", query)
      .load()
    inputDF.show()

    val modifiedDF = inputDF.select("value").withColumnRenamed("value", "number").
      withColumn("randomNumber", rand())

    // Performing Write operation to the collection 'sparkTestFromNumbers,
    // assigning number column as primary key


    modifiedDF.write.format("macrometa").option("federation", federation)
      .option("apiKey", apikey)
      .option("fabric", fabric)
      .option("collection", "sparkTestFromNumbers")
      .option("primaryKey", "number")
      .mode(SaveMode.Append).save()

    spark.close()




  }

  def greeting(): String = "Hello, world!"
}
