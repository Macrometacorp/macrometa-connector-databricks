/*
 * This Scala source file was generated by the Gradle 'init' task.
 */
package com.macrometa.spark

import org.apache.spark.sql.functions.rand
import org.apache.spark.sql.{SaveMode, SparkSession}

object CollectionTest extends App {

    val apikey = "apikey "
    val federation = "support.eng.macrometa.io"
    val fabric = "_system"
    val collection = "collection_1"
    val batchSize = 10
    val query = s"FOR doc IN $collection RETURN doc"

    val spark = SparkSession.builder().master("local[*]").getOrCreate()

    //This part is to read from the collection named numbers
    val inputDF = spark.read
      .format("com.macrometa.spark.collection.MacrometaTableProvider")
      .option("federation", federation)
      .option("apiKey", apikey)
      .option("fabric", fabric)
      .option("collection", collection)
      .option("batchSize", batchSize)
      .option("query", query)
      .load()
    inputDF.show()

    val modifiedDF = inputDF.select("value").withColumnRenamed("value", "number").
      withColumn("randomNumber", rand())

    // Performing Write operation to the collection 'sparkTestFromNumbers,
    // assigning number column as primary key


    modifiedDF.write.format("com.macrometa.spark.collection.MacrometaTableProvider").option("federation", federation)
      .option("apiKey", apikey)
      .option("fabric", fabric)
      .option("collection", "collection_2")
      .option("primaryKey", "number")
      .mode(SaveMode.Append).save()

    spark.close()

}
