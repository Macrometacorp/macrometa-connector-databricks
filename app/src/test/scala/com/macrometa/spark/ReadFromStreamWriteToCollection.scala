/*
 * This Scala source file was generated by the Gradle 'init' task.
 */
package com.macrometa.spark

import org.apache.spark.sql.{DataFrame, SaveMode, SparkSession}

object ReadFromStreamWriteToCollection extends App {

  val regionUrl = "*.macrometa.io"
  val port = "6651"
  val fabric = "_system"
  val tenant = "<YOUR_TENANT>"
  val replication = "global"
  val sourceStream = "<SOURCE_SAMPLE_STREAM_NAME>"
  val authToken = ""
  val sourceSubscription = "test-subscription-123"

  val sourceOptions = Map(
    "regionUrl" -> regionUrl,
    "token" -> authToken,
    "fabric" -> fabric,
    "tenant" -> tenant,
    "replication" -> replication,
    "stream" -> sourceStream,
    "subscriptionName" -> sourceSubscription
  )

  val spark = SparkSession
    .builder()
    .appName("MacrometaStreamingApp")
    .master("local[*]")
    .getOrCreate()

  val inputStream = spark.readStream
    .format("com.macrometa.spark.stream.MacrometaTableProvider")
    .options(sourceOptions)
    .load()


  val targetOptions = Map(
    "regionUrl" -> regionUrl,
    "apiKey" -> "apikey ",
    "fabric" -> fabric,
    "collection" -> "<YOUR_TARGET_COLLECTION>",
    "batchSize" -> 100.toString,
  )
  val query = inputStream.writeStream
    .foreachBatch { (batchDF: DataFrame, batchId: Long) =>
      batchDF.write
        .format("com.macrometa.spark.collection.MacrometaTableProvider")
        .options(targetOptions)
        .mode(SaveMode.Append)
        .save()
    }
    .option("checkpointLocation", "checkpoint")
    .start()

  query.awaitTermination()

}
